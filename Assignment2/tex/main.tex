%-------------------------------------------------------------------------------
% LATEX TEMPLATE ARTIKEL
%-------------------------------------------------------------------------------
% Dit template is voor gebruik door studenten van de de bacheloropleiding 
% Informatica van de Universiteit van Amsterdam.
% Voor informatie over schrijfvaardigheden, zie 
%                               https://practicumav.nl/schrijven/index.html
%
%-------------------------------------------------------------------------------
%	PACKAGES EN DOCUMENT CONFIGURATIE
%-------------------------------------------------------------------------------

\documentclass{uva-inf-article}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}

\usepackage[style=authoryear-comp]{biblatex}
\addbibresource{references.bib}

%-------------------------------------------------------------------------------
%	GEGEVENS VOOR IN DE TITEL, HEADER EN FOOTER
%-------------------------------------------------------------------------------

% Geef je artikel een logische titel die de inhoud dekt.
\title{Assignment 1: Estimating the area of the Mandelbrot set using Monte Carlo integration}

% Vul de naam van de opdracht in zoals gegeven door de docent en het type 
% opdracht, bijvoorbeeld 'technisch rapport' of 'essay'.
%\assignment{Naam van de opdracht}
%\assignmenttype{Type opdracht}

% Vul de volledige namen van alle auteurs in en de corresponderende UvAnetID's.
\authors{Alexander Künnen; Aaron De Clercq}
\uvanetids{UvAnetID 14101955; UvAnetID 14483610}

% Vul de naam van je tutor, begeleider (mentor), of docent / vakcoördinator in.
% Vermeld in ieder geval de naam van diegene die het artikel nakijkt!
\tutor{}
\mentor{}
\docent{Gábor Závodszky}

% Vul hier de naam van je tutorgroep, werkgroep, of practicumgroep in.
%\group{Naam van de groep}

% Vul de naam van de cursus in en de cursuscode, te vinden op o.a. DataNose.
\course{Stochastic simulations}
\courseid{}

% Dit is de datum die op het document komt te staan. Standaard is dat vandaag.
\date{\today}

%-------------------------------------------------------------------------------
%	VOORPAGINA 
%-------------------------------------------------------------------------------

\begin{document}
\maketitle

\justifying

%-------------------------------------------------------------------------------
%	INHOUDSOPGAVE EN ABSTRACT
%-------------------------------------------------------------------------------


%TC:ignore
%\tableofcontents
%\begin{abstract}
%\end{abstract}
%TC:endignore

%-------------------------------------------------------------------------------
%	INHOUD
%-------------------------------------------------------------------------------
% Hanteer bij benadering IMRAD: Introduction, Method, Results, Discussion.

\section{Introduction}

The Monte Carlo method is generally considered any approach that leads to a solution to a population-based problem by using a random sequence of numbers to represent a sample of the total population \parencite{halton1970}. It is a widely used method in the study of stochastic processes, like the behaviour of neutron chain reactions in fission devices \parencite{eckhardt1987} or the determination of efficiencies in gamma-ray detectors \parencite{raeside1976}.
The Monte Carlo method can also serve as a numerical integration technique to solve problems that are not necessarily stochastic. 
Compared to deterministic numerical integrators, such as the trapezoidal rule, the Monte Carlo integration method has a convergence rate independent of the dimensionality of the problem, indicating that it is a better technique for high-dimensional problems \parencite{james1980}.

Since the Monte Carlo integration method uses a random number generator, it is a non-deterministic method. Therefore, determining the quality as an estimator of the solution must be done statistically by looking at the variance.
Adding more sample points would reduce the variance and yield a better estimator, but this also increases the computational cost \parencite{james1980}.
Other techniques to reduce the variance, for example, stratified sampling and importance sampling, have been developed \parencite{james1980, kroese2012}.

To study the Monte Carlo integration method and the different variance reduction techniques in more detail, we will use them to estimate the area of the Mandelbrot set. The Mandelbrot set is the set of values in the complex plane for which the sequence,

\begin{equation}
    z_{n + 1} = z_n^2 + c
    \label{eq:mandelbrot}
\end{equation}

with $z_0$ equal to zero, remains bounded \parencite{ewing1992}.
Until now, there is no analytical expression to calculate the area of the Mandelbrot set, so the exact value of the area remains unknown \parencite{bittner2017}.
However, because the fraction of samples inside the Mandelbrot set is related to the area of the Mandelbrot set relative to the total sampling area, we can use the Monte Carlo integration technique to estimate the area.

In this process, there are two main approximations. The first one is the number of iterations for which we check if the sequence is bounded. For a value to be in the Mandelbrot set, equation \ref{eq:mandelbrot} should be bounded for every n, but we can only check this for a finite sequence. The second approximation is the number of sample points for which we check if the sequence is bounded. A particularly interesting question is how the estimate of the area changes if we vary the number of iterations or the number of sampling points.

Additionally, we can look at the effect of using stratified sampling techniques on the convergence rate. With Latin hypercube sampling \parencite{wei1996} and orthogonal sampling, the idea is to subdivide the total sampling area into subareas to get a more homogeneous sampling strategy. As these are known ways to reduce the variance, we can assume that the estimate of the area of the Mandelbrot set would improve compared to pure random sampling.

In the final part, we will test if a combination of stratified and importance sampling can further improve the convergence rate.

\section{Theory}
    
    \subsection{queuing schedules}
    Queue scheduling refers to a method in computational sciences in which a queuing 
    process is used to store a number of tasks. First in First Out-scheduling performs these tasks in order of arrival at that queue. With First in Last out scheduling the newest Task in the queue is chosen instead.\\
    Both take into account the assumption being that there is a specific order for the tasks to arrive at
    the queue.\\
    
    \subsection{Kendall's Notation}
    Kendall's Notation as used in the 1953 Paper of Kendall\parencite{Kendall1953} consist of 3 entries in the form of X/Y/n.
    X denotes the distribution of input signals/ arrivals of tasks in the queue, Y the distribution of 
    server-work time or the amount of time necessary to perform a task and n being the amount of server nodes 
    taking on tasks.
    Kendall noted several distributions in their paper\parencite{Kendall1953} .
    For the following A(u) is probability of a given task arriving after a given time u.\\
    D;Deterministic:
\begin{equation}\label{eq:determ}
	\begin{split}
		\text {Each task arrives at a given time($\mu$)}\\
        A(u) = \text { if  }u<\mu : 0 \text{ else : } 1
    \end{split}
	\end{equation}
    M;"random" or Poisson distributed
    \begin{equation}\label{eq:poisson}
    A(u) = 1-e^{-u/\mu}
	\end{equation}
    $E_k$; Erlangian
    \begin{equation}\label{eq:erlang}
        E_k \text { (Erlangian): } d A(u) \equiv \frac{(k / a)^k}{\Gamma(k)} e^{-k u / a} u^{k-1} d u\\
	\end{equation}
    G/GI; No assumption is taken for the distribution\\
    
    One example is the M/M/n $(n \in \mathbb{N})$ queuing simulation(Poisson distributed Arrivals and serverwork time with n servers) which we will focus on.



\subsection{System load}
The system load in the context of a M/M/n queuing simulation refers to the 

\subsection{Statistical tests}

To compare both the precision and the accuracy we need to apply two different tests, the F test for equality of variances and the Welch's t-test for equality of mean. In the F test, the F-value is equal to $S_x^2$/$S_y^2$, with $S_y^2$ and $S_x^2$ being the variances of the two sample groups to compare. Under the Null-hypothesis, this F-value has a so-called F-distribution. In our experiment, the Null-Hypothesis 
describes the case that the two sample groups have the same variance, and all differences in the variance are up to random effects.
In this report, we want to prove that our devised sampling method converges faster than the average Monte Carlo method for estimating the Mandelbrot area with orthogonal and Latin-hypercube sampling.
So, we want to show that the variance of our method is significantly smaller than the variance of the compared method by $\frac{S_{Monte-Carlo}^2}{S_{Strategic-sampling}^2}> F_{Critical Value}$ \parencite{Chatfield1980} 
This means that the likelihood of those two sample groups having the same variance is smaller than $5\%$(Fischer Constant/ significance level of 5\%) with a degree of freedom of n-1 in both sample groups and a specific $F_{Critical Value}$ .
The Welch's T-test is a version of the Students T-test in which the assumption that the two sample groups have a
similar variance is dropped. We get the critical value that needs to be surpassed to reject the null hypothesis by

\begin{equation*}
    T_f^{-1}(\frac{p+1}{2}),
\end{equation*}
with
\begin{equation*}
    f = \frac{(\frac{S_x^2}{n}+\frac{S_y^2}{n})^2}{\frac{S_x^4}{n^2(n-1)}+\frac{S_y^4}{n^2(n-1)}}
\end{equation*}
and the corresponding value obtained by the two samples to be
\begin{equation*}
\|\frac{\overline{X}-\overline{Y}}{\sqrt{\frac{S_x^2}{n}+\frac{S_y^2}{n}}}\|.
\end{equation*}


\subsection{correlation coefficient}

\section{Methodology}

\subsection{System load - Waiting time correlation}



\section{Results and discussion}
\subsection{Varying the number or iterations and sample points}

The left graph in figure \ref{fig:var_i_s} shows the estimated area of the Mandelbrot set as a function of the number of iterations, where the area is corrected with respect to the estimated area for the maximal number of iterations.
At low iterations, we see that the area of the Mandelbrot set is highly overestimated.
This is because many sample points are considered inside the Mandelbrot set because the value has a magnitude below 2 during the first few iterations of the sequence.
Increasing the number of iterations will lead to a reduction of the estimated area because values for which it takes a while before the sequence starts to diverge are correctly assigned as outside the set. 
This means that increasing the number of iterations will lead to higher accuracy of the estimation.\\

In the right graph of figure \ref{fig:var_i_s}, we keep the number of iterations constant and plot the average estimated area over 100 simulations as a function of the number of sample points.
The edges of the coloured area represent the average area $\pm$ the sample variance.
When using very few sample points, the average area is slightly underestimated because the sampling area is much bigger than the Mandelbrot area.
Increasing the number of sample points leads to a significant reduction in the variance between different simulations.

\begin{figure}[h]
    \centering
   \includegraphics[width=.85\textwidth]{graphs/fractal.pdf}
    \caption{Estimate of the area of the Mandelbrot set as a function of the number of iterations and samples.}
    \label{fig:var_i_s}
\end{figure}

\subsection{Effect of stratified sampling}

Figure \ref{fig:stratified} shows the average estimated area and the sample variance as a function of the number of sampling points.
When using a low number of sampling points, the Latin hypercube sampling method will have a higher estimated area compared to Latin hypercube and uniform sampling. Looking at the left graph \ref{fig:stratified_stats}, the difference in the average area between uniform and Latin hypercube / Latin hypercube and orthogonal sampling is only significant for the lowest number of sampling points.
Increasing the number of sampling points shows that all three sampling methods converge towards the same estimated area.\\

In the right graph of figure \ref{fig:stratified}, the sample variance is plotted as a function of the number of sampling points. The sample variance is clearly the highest for uniform sampling and orthogonal sampling results in the lowest variance. The results of an F-test, shown in the right graph of figure \ref{fig:stratified_stats}, indicate that the difference in the variance is significant for all the number of sampling points.
This indicates that orthogonal sampling is the better technique compared to Latin hypercube sampling to improve the variance reduction.

\begin{figure}[h]
    \centering
    \includegraphics[width=.85\textwidth]{graphs/fractal.pdf}
    \caption{Estimate of the area of the Mandelbrot set using different sampling strategies.}
    \label{fig:stratified}
\end{figure}

\begin{figure}[h]
    \centering
   \includegraphics[width=.85\textwidth]{graphs/fractal.pdf}
    \caption{Left: p-values of a Welch t-test as a function of the number of sampling points. Right: F-values as a function of the number of sampling points.}
    \label{fig:stratified_stats}
\end{figure}


\subsection{Strategic sampling to improve the convergence rate}
We compared the results of the Strategic sampling method against Latin hypercube/orthogonal sampling to determine whether the mean or variance is impacted by the choice of method. The exact Statistical tests were discussed in the Theory part. The Null Hypothesis was described as the results being virtually similar.

\subsubsection{Accuracy}

To determine if the accuracy of the results is affected we compared the means in figure \ref{fig:c_mean} showing the overlap in confidence intervals. The confidence intervals were created by adding the standard deviation of the sample group to the mean in that cycle. For orthogonal sampling, the confidence intervals overlap neatly whereas for Latin hypercube sampling the confidence intervals show distinct areas. In each case, the averages of the samples lie inside the confidence intervals of both methods.\\

\begin{figure}[h!]
  \centering
 \includegraphics[width=.85\textwidth]{graphs/fractal.pdf}
  \caption{Mean comparison with 1-sigma confidence intervals using Latin hypercube(right) and orthogonal sampling(left) when compared with Strategic Sampling. The dashed green Line represents the measured value for the Mandelbrot set area from \parencite{mitchell2001}}
  \label{fig:c_mean}
\end{figure}
  

The Welch's T-test, shown in figure \ref{fig:welch_t}, compares the Welch's T-value in each cycle against the required critical value, to reject the null hypothesis. The orthogonal sampling method shows one distinct value in all the cycles, otherwise, the critical value is not surpassed.\\
\begin{figure}[h!]
  \centering
  %\includegraphics[scale=0.4]{welch_t_test_noTitle.png}
  \includegraphics[width=.85\textwidth]{graphs/fractal.pdf}
  \caption{Welch's T-test comparing the mean distributions for Strategic sampling with Latin hypercube(right) and orthogonal sampling(left) with the critical T-value(red) with significance level 0.05}
  \label{fig:welch_t}
\end{figure}

The T test shows that the mean values are not separate for the different values. The one outlier in figure \ref{fig:welch_t} can be attributed to randomness since the experiment is repeated for several cycles and subsequent values are not affected. This suggests our method does not improve the accuracy of the results.


\subsubsection{Precision}

To find out if the precision of the result is affected by the method we compared the variances  (figure \ref{fig:log_var}) in the sample groups and performed a F-test (figure \ref{fig:f_test}) which was compared against the corresponding critical F-value for significance.
In figure \ref{fig:log_var}, the variance of the Latin hypercube sampling method was constantly higher than the Strategic sampling method, whereas the variance of the orthogonal sampling method had very similar results to our applied method.\\

\begin{figure}[h!]
  %\includegraphics[scale=0.4]{l}
  \centering
 \includegraphics[width=.85\textwidth]{graphs/fractal.pdf}
  \caption{Difference in variance in between Strategic sampling(red) and Latin hypercube(yellow)/ orthogonal sampling(blue) as shown on a logarithmic scale}
  \label{fig:log_var}
\end{figure}

In the F-test (figure \ref{fig:f_test}), the F-value for the Latin hypercube sampling comparison is always above the critical value.
The orthogonal Sampling comparison lies close to the critical value for the most part but shows two occasions clearly surpassing the value in a cycle.\\

\begin{figure}[h!]
  \centering
  %\includegraphics[scale=0.4]{one_sided_f_test_noTitle.png}
  \includegraphics[width=.85\textwidth]{graphs/fractal.pdf}
  \caption{F-test comparing the variance distributions for Strategic sampling with Latin hypercube(right) and orthogonal sampling(left) with the critical F-value(red) with significance level 0.05 as shown on a logarithmic scale}
  \label{fig:f_test}
\end{figure}

This shows that our method improved the variance and therefore the precision compared to Latin hypercube sampling since the F-test suggests two differing variances while the variance is higher for Latin hypercube sampling (figure \ref{fig:log_var}). On the other hand, orthogonal sampling still converges either similarly fast or even faster than Strategic sampling as suggested by their respective plots in figure \ref{fig:log_var} and \ref{fig:f_test}.


\newpage
\section{Conclusions}
In conclusion, we were able to show that increasing the number of iterations to check for divergence in the Mandelbrot set improved the overall accuracy of the result, specifically reduced the resulting area, while the number of samples used for calculating the result in a Monte Carlo simulation increased the precision/ decreased the variance in the resulting area estimations.\\
Also when comparing sampling methods uniform, Latin hypercube and orthogonal we found no evidence suggesting they affect the overall accuracy but found the Latin hypercube sampling method to improve the precision compared to uniform sampling and orthogonal sampling to improve the precision compared to Latin hypercube sampling.\\
Furthermore, we improved the precision of the Monte Carlo Method estimating the area of the Mandelbrot set using Strategic sampling compared to Latin hypercube sampling which it was based on, but could not affect the accuracy of the result similar to previous results.
The orthogonal Sampling Method on the other hand was able either outperform or contest the Strategic Sampling approach by itself. \\
Future Precision improvements seem plausible when applying orthogonal sampling to the sampling methods in the subareas instead of Latin hypercube sampling. Due to time constraints, this approach could not be investigated in this report.

%-------------------------------------------------------------------------------
%	REFERENTIES
%-------------------------------------------------------------------------------
\clearpage
\printbibliography

%-------------------------------------------------------------------------------
%	BIJLAGEN 
%-------------------------------------------------------------------------------

%TC:ignore
%\appendix 
%\section{Bijlage {\LaTeX} code}
%Bijgevoegd zijn de \textattachfile{main.tex}{code} en 
%\textattachfile{references.bib}{bibliografie}.
%TC:endignore

%-------------------------------------------------------------------------------
\end{document}